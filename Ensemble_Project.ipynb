{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit (windows store)",
      "metadata": {
        "interpreter": {
          "hash": "7831c92f7874e068193c127183976751b071e9d9926ab3fb2f4632953da558c0"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Ensemble Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIFj8NxvoSk4",
        "outputId": "cac8d02d-2c85-4671-c758-0581d8551168"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "oIFj8NxvoSk4",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ordinary-butterfly"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt, ceil\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "ordinary-butterfly",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caroline-demographic"
      },
      "source": [
        "# Open data file\n",
        "with open(\"/content/drive/MyDrive/CNN/data2.pickle\", 'rb') as f:\n",
        "    data = pickle.load(f, encoding=\"latin1\")\n",
        "\n",
        "# Prepare for keras\n",
        "data['y_train'] = to_categorical(data['y_train'], num_classes=43)\n",
        "data['y_validation'] = to_categorical(data['y_validation'], num_classes=43)\n",
        "\n",
        "# Making channels come at the end\n",
        "data['x_train'] = data['x_train'].transpose(0, 2, 3, 1)\n",
        "data['x_validation'] = data['x_validation'].transpose(0, 2, 3, 1)\n",
        "data['x_test'] = data['x_test'].transpose(0, 2, 3, 1)"
      ],
      "id": "caroline-demographic",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRfmrCyyIjFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2acd89e2-35e2-4992-ab7e-886e4d29d6fc"
      },
      "source": [
        "filters = [3, 5, 9]\n",
        "model = [0] * len(filters)\n",
        "\n",
        "for i in range(len(model)):\n",
        "  model[i] = Sequential()\n",
        "  model[i].add(Conv2D(32, kernel_size=filters[i], padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(MaxPool2D(pool_size=2))\n",
        "  model[i].add(Conv2D(256, kernel_size=filters[i], padding='same', activation='relu'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(MaxPool2D(pool_size=2))\n",
        "  model[i].add(Flatten())\n",
        "  model[i].add(Dense(500, activation='relu'))\n",
        "  model[i].add(Dropout(0.5))\n",
        "  model[i].add(Dense(500, activation='relu'))\n",
        "  model[i].add(Dropout(0.5))\n",
        "  model[i].add(Dense(43, activation='softmax'))\n",
        "  model[i].compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'] )\n",
        "\n",
        "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\n",
        "epochs = 5\n",
        "\n",
        "h = [0] * len(model)\n",
        "for i in range(len(h)):\n",
        "  h[i]= model[i].fit(data['x_train'], data['y_train'],\n",
        "                    batch_size=500, epochs = epochs,\n",
        "                    validation_data = (data['x_validation'], data['y_validation']),\n",
        "                    callbacks=[annealer], verbose=1)\n",
        "  print('Model with filters {0:d}x{0:d}, epochs={1:d}, training accuracy={2:.5f}, validation accuracy={3:.5f}'.\\\n",
        "      format(filters[i], epochs, max(h[i].history['accuracy']), max(h[i].history['val_accuracy'])))"
      ],
      "id": "iRfmrCyyIjFK",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "174/174 [==============================] - 15s 82ms/step - loss: 3.1348 - accuracy: 0.2627 - val_loss: 7.6086 - val_accuracy: 0.0068\n",
            "Epoch 2/5\n",
            "174/174 [==============================] - 14s 79ms/step - loss: 0.6346 - accuracy: 0.8007 - val_loss: 7.6674 - val_accuracy: 0.0510\n",
            "Epoch 3/5\n",
            "174/174 [==============================] - 14s 79ms/step - loss: 0.2885 - accuracy: 0.9090 - val_loss: 3.4091 - val_accuracy: 0.3741\n",
            "Epoch 4/5\n",
            "174/174 [==============================] - 14s 79ms/step - loss: 0.1819 - accuracy: 0.9431 - val_loss: 0.1982 - val_accuracy: 0.9406\n",
            "Epoch 5/5\n",
            "174/174 [==============================] - 14s 79ms/step - loss: 0.1392 - accuracy: 0.9574 - val_loss: 0.1321 - val_accuracy: 0.9639\n",
            "Model with filters 3x3, epochs=5, training accuracy=0.95973, validation accuracy=0.96395\n",
            "Epoch 1/5\n",
            "174/174 [==============================] - 19s 105ms/step - loss: 3.2125 - accuracy: 0.2585 - val_loss: 5.2714 - val_accuracy: 0.0438\n",
            "Epoch 2/5\n",
            "174/174 [==============================] - 18s 104ms/step - loss: 0.5249 - accuracy: 0.8400 - val_loss: 4.6350 - val_accuracy: 0.1027\n",
            "Epoch 3/5\n",
            "174/174 [==============================] - 18s 104ms/step - loss: 0.2300 - accuracy: 0.9298 - val_loss: 1.9392 - val_accuracy: 0.5209\n",
            "Epoch 4/5\n",
            "174/174 [==============================] - 18s 104ms/step - loss: 0.1319 - accuracy: 0.9595 - val_loss: 0.2088 - val_accuracy: 0.9338\n",
            "Epoch 5/5\n",
            "174/174 [==============================] - 18s 103ms/step - loss: 0.0924 - accuracy: 0.9730 - val_loss: 0.0865 - val_accuracy: 0.9746\n",
            "Model with filters 5x5, epochs=5, training accuracy=0.97351, validation accuracy=0.97460\n",
            "Epoch 1/5\n",
            "174/174 [==============================] - 31s 160ms/step - loss: 3.2897 - accuracy: 0.2313 - val_loss: 5.5611 - val_accuracy: 0.0950\n",
            "Epoch 2/5\n",
            "174/174 [==============================] - 24s 135ms/step - loss: 0.7292 - accuracy: 0.7793 - val_loss: 4.4522 - val_accuracy: 0.1279\n",
            "Epoch 3/5\n",
            "174/174 [==============================] - 23s 135ms/step - loss: 0.3338 - accuracy: 0.8983 - val_loss: 0.7347 - val_accuracy: 0.7964\n",
            "Epoch 4/5\n",
            "174/174 [==============================] - 23s 134ms/step - loss: 0.2203 - accuracy: 0.9345 - val_loss: 0.3057 - val_accuracy: 0.9195\n",
            "Epoch 5/5\n",
            "174/174 [==============================] - 23s 134ms/step - loss: 0.1719 - accuracy: 0.9491 - val_loss: 0.3361 - val_accuracy: 0.9295\n",
            "Model with filters 9x9, epochs=5, training accuracy=0.95280, validation accuracy=0.92948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-99YhFg7k_TM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc0647e-ebb8-4156-95ce-8dfccff00af4"
      },
      "source": [
        "import scipy.stats as sciStats\n",
        "\n",
        "labels = []\n",
        "for i in range(len(model)):\n",
        "    temp = model[i].predict(data['x_test'])\n",
        "    temp = np.argmax(temp, axis=1)\n",
        "    labels.append(temp)\n",
        "    \n",
        "    # We compare predicted class with correct class for all input images\n",
        "    # And calculating mean value among all values of following numpy array\n",
        "    # By saying 'testing_accuracy == data['y_test']' we create numpy array with True and False values\n",
        "    # 'np.mean' function will return average of the array elements\n",
        "    # The average is taken over the flattened array by default\n",
        "    temp = np.mean(temp == data['y_test'])*100\n",
        "    \n",
        "    print('data2 filter {0:d} testing accuracy = {1:.5f}%'.format(filters[i], temp))\n",
        "\n",
        "# Ensemble with voting\n",
        "labels = np.array(labels)\n",
        "# labels = np.transpose(labels, (1, 0))\n",
        "labels = sciStats.mode(labels)[0]\n",
        "labels = np.squeeze(labels)\n",
        "# np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "ensembleAccuracy = np.mean(labels == data['y_test'])*100\n",
        "\n",
        "print('Ensemble Accuracy = {0:.5f}%'.format(ensembleAccuracy))"
      ],
      "id": "-99YhFg7k_TM",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data2 filter 3 testing accuracy = 95.24941%\n",
            "data2 filter 5 testing accuracy = 95.35234%\n",
            "data2 filter 9 testing accuracy = 91.52811%\n",
            "Ensemble Accuracy = 95.84323%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}